{
 "metadata": {
  "name": "",
  "signature": "sha256:ccd34c21695bbf38f193cde83ee1037b5809058e1e0fbcb075adb610cc7dcef3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "%pylab inline\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"breast_cancer.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We need to pop out malignant because that is what we are predicting and assign it to y.\n",
      "y = df.pop(\"malignant\")\n",
      "#We drop the key and id number columns and create x.\n",
      "x = df.drop([\"Unnamed: 0\", \"id number\"], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RandomForestClassifier(n_estimators=500, oob_score=False, n_jobs=-1, random_state=42, max_features=0.2, min_samples_leaf=1)\n",
      "model.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features=0.2,\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=500, n_jobs=-1,\n",
        "            oob_score=False, random_state=42, verbose=0)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"AUC: \", roc_auc_score(y_test, model.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AUC:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.967251461988\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>This is lower than the AUC score I got in my assignment.  I used the probability predict though. I can't use probability predict for accuracy.  </b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"AUC: \", roc_auc_score(y_test, model.predict_proba(x_test)[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AUC:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.996725146199\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b> I want to be sure I used the best Random Forest Classifier then.  Maybe I was wrong.  </b>  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_estimators = [300,400,500,600,700]\n",
      "max_features = ['auto', 'sqrt','log2']\n",
      "min_samples_split = [1,2,3,5,7]\n",
      "\n",
      "\n",
      "rfc = RandomForestClassifier(n_jobs=1)\n",
      "#Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:\n",
      "estimator = GridSearchCV(rfc,\n",
      "                         dict(n_estimators=n_estimators,\n",
      "                              max_features=max_features,\n",
      "                              min_samples_split=min_samples_split\n",
      "                              ), cv=None, n_jobs=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=-1,\n",
        "       param_grid={'min_samples_split': [1, 2, 3, 5, 7], 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [300, 400, 500, 600, 700]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best_rfc = estimator.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"AUC: \", roc_auc_score(y_test, best_rfc.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AUC:  0.956140350877\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b> Well, I guess I had the best rfc afterall. Moving on. </b>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = accuracy_score(y_test, best_rfc.predict(x_test))\n",
      "print \"Accuracy: \", accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy:  0.964285714286\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classification_report(y_test, best_rfc.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.97      0.98      0.97        95\n",
        "          1       0.95      0.93      0.94        45\n",
        "\n",
        "avg / total       0.96      0.96      0.96       140\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_validation.cross_val_score(best_rfc, x, y, cv=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([ 0.92957746,  0.97142857,  0.97142857,  0.91428571,  0.98571429,\n",
        "        0.98571429,  0.97142857,  0.98571429,  0.98550725,  1.        ])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_score = scores.mean()\n",
      "std_dev = scores.std()\n",
      "std_error = scores.std() / math.sqrt(scores.shape[0])\n",
      "ci =  2.262 * std_errorlower = mean_score - ci\n",
      "upper = mean_score + ci\n",
      "\n",
      "print \"Score is %f +/-  %f\" % (mean_score, ci)\n",
      "print '95 percent probability that if this experiment were repeated over and over the average score would be between %f and %f' % (lower, upper)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score is 0.970080 +/-  0.018412\n",
        "95 percent probability that if this experiment were repeated over and over the average score would be between 0.951668 and 0.988492\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>AUC & K-Fold</h3>\n",
      "My AUC score (without probability) fell on the lower end of the range, which for our example of breat cancer detecting is probably better because we don't want to be over confident.<br>\n",
      "<h3>Accuracy</h3>\n",
      "Based on the results, we will accurately predict the malignancy of tumors 96% of the time.  So 4% of the time, we will not be correct in diagnosis whether diagnosing cancer when there isn't cancer, missing cancer when there is cancer.<br>\n",
      "<h3>Precision</h3>\n",
      "For benign (0) predictions, we had a precision score of .97 on 95 observations.  This means that we predict 92 people to have benign tumors correctly.  However, there are 3 people that our model will predict to have a benign tumor when in fact it is malignant<br>\n",
      "For malignant tumors we had a precision score of .95 which means 43 times we predict a malignant tumor correctly.  2 times we predict a benign tumor as malignant.  <br>\n",
      "<h3>Recall</h3>\n",
      "The recall score for benign tumors was .98.  So of the 95 people we identified as having benign tumors, 93 of them will actually have benign tumors.   The score for malignant is much lower.  .93.  Of the 45 people we identify to have malignant tumors, 43 will actually have malignant tumors.  <br>\n",
      "\n",
      "<h3>Model Performance</h3>\n",
      "Our AUC score means that for every threshold (grouping) that we choose in our model, we will have an overall true to false positive rate of 95.6%  For predicting cancer, this may not be the best.  For our sample size we saw that telling someone it was benign when in fact it was malignant only affected 3 people.  This doesn't seem like many, but what if we increased by 10 times, or 100.  It could be an issue.  Based on our k-fold analysis we could see as high as 98.85 percent which would be acceptable.  But in my opinion we want to be in the 99 percent range when informing patients on something as serious as cancer.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}